{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание 2\n",
    "### Линейные модели\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 1 (Теоретическое)\n",
    "На лекции было сказано, что большие по модулю веса могут сигнализировать о наличии переобучения или о наличии линейной зависимости между признаками. В этом задании вам предстоит показать, что при отсутствии регуляризации (штрафа за большие веса) коэффициенты перед зависимыми признаками могут расти неограниченно.\n",
    "\n",
    "Рассмотрим данные, в которых истинная зависимость целевой переменной $y$ от признака $x$ описывается коэффициентом w (то есть истинная зависимость выглядит как $y = wx + ...$, где ... - другие признаки НЕ ЗАВИСЯЩИЕ от x). Искусственно добавим в модель зависимый признак равный -x. \n",
    "После обучения модели получим зависимость $y = w_1x + w_2 (-x) + ...$.\n",
    "\n",
    "Покажите, что веса $w_1$ и $w_2$ могут быть сколь угодно большими."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Ваш ответ здесь> (для ответа кликните дважды по этому тексту)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Линейная регрессия с L2-регуляризацией или Ridge регрессия состоит в минимизации следующего функционала:\n",
    "$$||Y - Xw||^2 + \\lambda ||w||^2 \\rightarrow min$$\n",
    "\n",
    "Для данной задачи можно выписать явное решение:\n",
    "$$w = (X^TX + \\lambda I)^{-1}X^TY$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте свой класс для гребневой регрессии, воспользовавшись формулой выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRidgeRegression:\n",
    "    \n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Принимает на вход X, y и вычисляет веса по данной выборке\n",
    "        # Не забудьте про фиктивный признак равный 1!\n",
    "        \n",
    "        # ВАШ КОД ЗДЕСЬ\n",
    "        # Коэффициент лямбда сохранен в переменной self.alpha\n",
    "        # Вам может понадобиться функция np.eye\n",
    "        \n",
    "        self.w = <...>\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # Принимает на вход X и возвращает ответы модели\n",
    "        \n",
    "        # ВАШ КОД ЗДЕСЬ\n",
    "\n",
    "        y_pred = <...>\n",
    "        return y_pred\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируем наш алгоритм на искусственных данных и сравним с линейной регрессией без регуляризации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regr = LinearRegression()\n",
    "ridge_regr = MyRidgeRegression(alpha=0.1)\n",
    "\n",
    "linear_regr.fit(X_train, y_train)\n",
    "ridge_regr.fit(X_train, y_train)\n",
    "\n",
    "linear_regr_predictions = linear_regr.predict(X_test)\n",
    "ridge_regr_predictions = ridge_regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метрики качества воспользуемся MAE (средняя абсолютная ошибка)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Linear regression: ', mean_absolute_error(y_test, linear_regr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ridge regression: ', mean_absolute_error(y_test, ridge_regr_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуйте подобрать такой коэффициент регуляризации, чтобы ошибка на тесте была меньше, чем у модели без регуляризации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 3\n",
    "В этом задании предлагается поиграться с различными ядрами SVM и подобрать лучшие параметры для каждого из них"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вам дан датасет (находится в файле banknote.data), в котором представлены измерения различных купюр. Задача состоит в их классификации на подлинные и фальшивые."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('banknote.data', sep=',', header=-1)\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделите выборку на трейн и тест в соотношении 1:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ВАШ КОД ЗДЕСЬ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого из ядер svc (параметр kernel='linear', 'poly', 'rbf') постарайтесь подобрать наилучший набор параметров, который максимизирует метрику F1-score на тесте. \n",
    "\n",
    "НАПОМИНАНИЕ: $ F1 = 2\\frac{precision \\cdot recall}{precision + recall} $\n",
    "\n",
    "ПОДСКАЗКА: параметр C в SVC отвечает за коэффициент регуляризации, degree - степень полинома в полиномиальном ядре, gamma - коэффициент в экспоненте у гауссовского ядра. Про другие парамеры (если понадобятся) предлагается прочитать самостоятельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ВАШ КОД ЗДЕСЬ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### БОНУС"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предлагаю немного расслабиться и посмотреть как делается полиномиальная регрессия (добавление степенных признаков в модель) и как Lasso проводит отбор признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation\n",
    "\n",
    "def gen_batch(n, w, beta):\n",
    "    d = len(w)\n",
    "    X = np.random.uniform(-10, 10, (n, 1))\n",
    "    X = np.sort(X, axis=0)\n",
    "    X = np.hstack([X ** i for i in range(d)])\n",
    "    t = X.dot(w) + np.random.normal(size=n) / beta ** 0.5\n",
    "    return X, t\n",
    "\n",
    "n = 200\n",
    "d = 21\n",
    "w_true = np.zeros(d)\n",
    "w_true[1] = 100\n",
    "w_true[3] = -1\n",
    "beta_true = 1e-4\n",
    "\n",
    "X_train, t_train = gen_batch(n, w_true, beta_true)\n",
    "X_test, t_test = gen_batch(n, w_true, beta_true)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(X_train[:, 1], t_train, s=3, label='Train data', alpha=0.3)\n",
    "plt.scatter(X_test[:, 1], t_test, s=3, label='Test data', alpha=0.3)\n",
    "plt.plot(X_train[:, 1], X_train.dot(w_true), label='Ground truth')\n",
    "\n",
    "plt.axes().set_xlabel('x')\n",
    "plt.axes().set_ylabel('y')\n",
    "plt.legend(ncol=3, loc=9, bbox_to_anchor=(0.5, 1.15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression with Cross-Validation\n",
    "from sklearn.linear_model import RidgeCV\n",
    "ridge = RidgeCV(cv=20, alphas=10.**np.arange(-6, 3, 1),\n",
    "                fit_intercept=False).fit(X_train, t_train)\n",
    "w_ridge = ridge.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regression with Cross-Validation\n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "lasso = LassoCV(cv=20, alphas=10.**np.arange(-6, 3, 1),\n",
    "                fit_intercept=False).fit(X_train, t_train)\n",
    "w_lasso = lasso.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_error(X, t, w):\n",
    "    return np.sum((X.dot(w.ravel()) - t) ** 2)\n",
    "\n",
    "# Comparison\n",
    "print('Ridge Regression')\n",
    "print('Features remaining:', np.sum(np.abs(w_ridge) > 1e-15), '/', d)\n",
    "print('Train error:', l2_error(X_train, t_train, w_ridge) / n)\n",
    "print('Test error: ', l2_error(X_test, t_test, w_ridge) / n)\n",
    "print('-'*50)\n",
    "print('Lasso Regression')\n",
    "print('Features remaining:', np.sum(np.abs(w_lasso) > 1e-15), '/', d)\n",
    "print('Train error:', l2_error(X_train, t_train, w_lasso) / n)\n",
    "print('Test error: ', l2_error(X_test, t_test, w_lasso) / n)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(X_train[:, 1], t_train, s=3, label='Train data', alpha=0.3)\n",
    "plt.scatter(X_test[:, 1], t_test, s=3, label='Test data', alpha=0.3)\n",
    "plt.plot(X_train[:, 1], X_train.dot(w_true), label='Ground truth')\n",
    "plt.plot(X_train[:, 1], X_train.dot(w_ridge), label='Ridge')\n",
    "plt.plot(X_train[:, 1], X_train.dot(w_lasso), label='Lasso')\n",
    "\n",
    "plt.axes().set_xlabel('x')\n",
    "plt.axes().set_ylabel('y')\n",
    "plt.legend(ncol=3, loc=9, bbox_to_anchor=(0.5, 1.25))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
